{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算机视觉纳米学位项目\n",
    "\n",
    "## 实战项目：图像标注\n",
    "\n",
    "---\n",
    "\n",
    "在这个notebook中，你要做的是训练你的CNN-RNN模型。\n",
    "\n",
    "我们欢迎并鼓励你在搜索好的模型时尝试多种不同的架构和超参数。\n",
    "\n",
    "这样的话，很有可能会使项目变得非常凌乱！所以，在提交项目之前，请确保清理以下内容：\n",
    "- 你在这个notebook上写的代码。该notebook应描述如何训练单个CNN-RNN架构，使其与你最终选择的超参数相对应。此外，你的notebook应便于审阅专家通过运行此notebook中的代码来复制结果。\n",
    "- **Step 2**中代码单元格的输出。这个输出显示的应该是从零开始训练模型时获得的输出。\n",
    "\n",
    "我们将会对这个notebook**进行评分**。\n",
    "\n",
    "你可以通过点击以下链接导航到该notebook：\n",
    "- [Step 1](#step1): 训练设置\n",
    "- [Step 2](#step2): 训练你的模型\n",
    "- [Step 3](#step3): (可选）验证你的模型\n",
    "\n",
    "<a id='step1'></a>\n",
    "## Step 1: 训练设置\n",
    "\n",
    "在该notebook的此步骤中，你需要通过定义超参数并设置训练过程中重要的其他选项来自定义对CNN-RNN模型的训练。在下面的**Step 2**中训练模型时，会使用到现在设置的值。\n",
    "\n",
    "请注意，你只可以修改以`TODO`语句开头的代码块。**对于所以不在`TODO`语句之前的代码块，不能做任何修改。**\n",
    "\n",
    "### 任务 #1\n",
    "\n",
    "首先，请设置以下变量：\n",
    "- `batch_size` - 每个训练批次的批次大小。它是指用于在每个训练步骤中修改模型权重的图像标注对的数量。\n",
    "- `vocab_threshold` - 单词阈值最小值。请注意，阈值越大，词汇量越小，而阈值越小，则表示将包括较少的词汇，词汇量则越大。\n",
    "- `vocab_from_file` - 一个布尔值，用于决定是否从文件加载词汇表。\n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  图像和单词嵌入的维度。\n",
    "- `hidden_size` - RNN解码器隐藏状态下的特征数。\n",
    "- `num_epochs` - 训练模型的epoch数。我们建议你设置为`num_epochs=3`，但可以根据需要随意增加或减少此数字。 [这篇论文](https://arxiv.org/pdf/1502.03044.pdf) 在一个最先进的GPU上对一个标注生成模型训练了3天，但很快你就会发现，其实在几个小时内就可以得到合理的结果！（_但是，如果你想让你的模型与当前的研究一较高下，则需要更长时间的训练。_)\n",
    "- `save_every` - 确定保存模型权重的频率。我们建议你设置为`save_every=1`，便于在每个epoch后保存模型权重。这样，在第`i`个epoch之后，编码器和解码器权重将在`models/`文件夹中分别保存为`encoder-i.pkl`和`decoder-i.pkl`。\n",
    "- `print_every` - 确定在训练时将批次损失输出到Jupyter notebook的频率。请注意，训练时，你**将不会**看到损失函数的单调减少，这一点非常好并且完全可以预料到！我们建议你将其保持在默认值`100` ，从而避免让这个notebook运行变慢，但之后随时都可以进行更改。\n",
    "- `log_file` - 包含每个步骤中训练期间的损失与复杂度演变过程的的文本文件的名称。\n",
    "\n",
    "对于上述某些值，如果你不确定从哪里开始设置，可以仔细阅读 [这篇文章](https://arxiv.org/pdf/1502.03044.pdf) 与 [这篇文章](https://arxiv.org/pdf/1411.4555.pdf) ，获得有用的指导！为了避免在该notebook上花费太长时间，我们建议你查阅这些研究论文，从中可以获得有关哪些超参数可能最有效的初始猜测。然后，训练单个模型，然后继续下一个notebook（**3_Inference.ipynb**）。如果你对模型的效果不满意，可以返回此notebook调整超参数和/或**model.py**中的体系结构，并重新训练模型。\n",
    "\n",
    "### 问题1\n",
    "\n",
    "**问题:** 详细描述你的CNN-RNN架构。对于这种架构，任务1中变量的值，你是如何选择的？如果你查阅了某一篇详细说明关于成功实现图像标注生成模型的研究论文，请提供该参考论文。\n",
    "\n",
    "**答案:** 编码器CNN用resnet50预训练模型进行迁移学习，去除顶部全连接层之后，增加了自定义的Flattern和全连接层，处理得到特征向量。在解码器RNN中，首先是一个嵌入层将说明中的每个词转化为一致形状的向量，再对之前CNN得到的特征向量应用相同的转化，然后进入lstm层。在每个时间步的单词作为输入，与lstm单元的隐藏状态结合生成输出，再经过一个全连接层，形成下个单词的概率分布。再将下个单词作为输入直到结束。$^1$\n",
    "\n",
    "变量说明：\n",
    "\n",
    "+ batch_size: batch_size较大时速度更快，噪声小，但需要的内存也更大；较小的batch_size可能带来噪声，但也可能由此越过误差曲线的局部最低点而更接近全局最低点。按照课程的推荐先选择32。\n",
    "+ vocab_threshold：5。先参考题干中提供的一篇文章做尝试$^2$。题干可能翻译错了，我理解是按词频设置阈值，该阈值以上的才会收入词汇表中供后续使用，如果阈值太小，可能会“收录一些生癖字”（will include rarer words）而不是“包括较少的词汇”，由于词频太低的词都被收录所以词汇量应该是大的。我也不熟悉一般阈值设为多少更合适，所以先试用参考文献中的阈值。\n",
    "+ vocab_from_file：从文件加载词汇表。\n",
    "+ embed_size：512。同样参考这篇文章做尝试$^2$。\n",
    "+ hidden_size: 512。同上$^2$。\n",
    "+ num_epochs: 5。条件允许的话应该尽可能大，只要Loss还在下降就不停。但此处资源有限，并且前期观察到当num_epochs为3的时候变化就已经很很缓慢了，可能已经近乎收敛。训练正式模型可以再搞高一点试试。\n",
    "+ save_every：1，保险，方便后续评估每个epoch截面的模型预测效果。\n",
    "+ print_every: 100。便于及时观察训练情况。\n",
    "+ log_file: 常规日志文件名“log.txt”。\n",
    "\n",
    "*Reference:*  \n",
    "*1.Lesson 7 图像说明，5~9.*  \n",
    "*2.Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and Tell: A Neural Image Caption Generator. arXiv:1411.4555 [cs] (2014).*\n",
    "\n",
    "### （可选）任务 #2\n",
    "\n",
    "请注意，我们为你推荐了一个用于预处理训练图像的图像转换`transform_train`，但同时，也欢迎并鼓励你根据需要进行修改。修改此转换时，请牢记：\n",
    "- 数据集中的图像具有不同的高度和宽度\n",
    "- 如果使用预先训练的模型，则必须执行相应的相应归一化。\n",
    "\n",
    "### 问题2\n",
    "\n",
    "**问题:** 你是如何在`transform_train`中选择转换方式的？如果你将转换保留为其提供的值，为什么你的任务它非常适合你的CNN架构？\n",
    "\n",
    "**答案:**  保留原来的转换方式。这里面最重要的就是使各种尺寸的训练图像变成和预训练模型一样的大小（`.RandomCrop(224)`），使其有同样的输入值，再转换成张量（`.ToTensor()`），并且归一化，使其具有与预训练网络匹配的平均值和标准差（`.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))`）。本任务中使用的预训练模型是`resnet50`，这些参数已经合适。其余的图像增强，例如随机水平翻转（`.RandomHorizontalFlip()`）等，是为了使模型具有更好的泛化能力，可酌情增减。如果要添加的话，或许可以加上垂直翻转、随机旋转一定角度等等，不过我认为在本次任务中不需要这么复杂，所以不改了。\n",
    "\n",
    "*Refernce:*  \n",
    "*https://github.com/pytorch/vision/issues/39*\n",
    "\n",
    "### 任务 #3\n",
    "\n",
    "接下来，你需要定义一个包含模型的可学习参数的Python列表。 例如，如果你决定使解码器中的所有权重都是可训练的，但只想在编码器的嵌入层中训练权重，那么，就应该将`params`设置为：\n",
    "\n",
    "```python\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### 问题3\n",
    "\n",
    "**问题:** 你是如何选择该架构的可训练参数的？ 为什么你认为这是一个不错的选择？\n",
    "\n",
    "**答案:** 如题干所示选用解码器中的所有权重和编码器的嵌入层。因为编码器的前半部分采用了预训练网络resnet50，都是已经训练过的，图像的许多特征都基本相似所以不必重复训练，况且要耗费很多资料。我们只需要训练自己编写的嵌入层和解码器即可。\n",
    "\n",
    "### 任务 #4\n",
    "\n",
    "最后，选择一个 [优化程序](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer)。\n",
    "\n",
    "### 问题4\n",
    "\n",
    "**问题:** 你是如何选择用于训练模型的优化程序的？\n",
    "\n",
    "**答案:** 优化器选择Adam，通常认为它的鲁棒性比较好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 765/414113 [00:00<01:55, 3563.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:34<00:00, 4394.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 32          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: 训练你的模型\n",
    "\n",
    "在**Step 1**中执行代码单元格后，下面的训练过程应该就不会出现问题了。\n",
    "\n",
    "在这里，完全可以将代码单元格保留其原样，无需修改即可训练模型。但是，如果要修改用于训练下面模型的代码，则必须确保审阅专家能够很容易地看明白你的更改内容。换句话说，请务必提供适当的注释来描述代码的工作方式！\n",
    "\n",
    "你可能会发现，使用加载已保存的权重来恢复训练很有用。在这种情况下，请注意包含你要加载的编码器和解码器权重的文件的名称（`encoder_file`和`decoder_file`）。之后，你就可以使用下面的代码行加载权重："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在试验参数时，请务必记录大量笔记并记录你在各种训练中使用的设置。特别是，你不希望遇到这样的情况，即已经训练了几个小时的模型，但不记得使用的设置:)。\n",
    "\n",
    "### 关于调整超参数的说明\n",
    "\n",
    "为了弄清楚模型的运行情况，你可以尝试去了解训练过程中训练损失和复杂度是如何演变的。为了做好本项目，我们建议你根据这些信息修改超参数。\n",
    "\n",
    "但是，这样你还是无法知道模型是否过度拟合训练数据，但你要知道的是，过度拟合是训练图像标注模型时常会遇到的问题。\n",
    "\n",
    "对于这个项目，你不必担心过度拟合。**该项目对模型的性能没有严格的要求**，你只需要证明你的模型在生成基于测试数据的标注时学到了**_一些东西_**。现在，我们强烈建议你为我们建议的3个epoch训练你的模型，但不必担心性能；然后，立即转换到下一个notebook（**3_Inference.ipynb**），查看模型对测试数据的执行情况。如果你的模型需要更改，可以回到这个notebook，修改超参数（如有必要的话），并重新训练该模型。\n",
    "\n",
    "也就是说，如果你想在这个项目中有所超越，可以阅读 [本文](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636)4.3.1节中最小化过度拟合的一些方法。在本notebook的下一个（可选）步骤中，我们提供了一些关于评估验证数据集性能的指导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/12942], Loss: 3.6723, Perplexity: 39.3421\n",
      "Epoch [1/5], Step [200/12942], Loss: 3.5924, Perplexity: 36.31975\n",
      "Epoch [1/5], Step [300/12942], Loss: 3.4604, Perplexity: 31.8286\n",
      "Epoch [1/5], Step [400/12942], Loss: 3.1693, Perplexity: 23.79063\n",
      "Epoch [1/5], Step [500/12942], Loss: 2.6174, Perplexity: 13.6996\n",
      "Epoch [1/5], Step [600/12942], Loss: 3.1111, Perplexity: 22.44687\n",
      "Epoch [1/5], Step [700/12942], Loss: 3.3846, Perplexity: 29.5054\n",
      "Epoch [1/5], Step [800/12942], Loss: 2.9766, Perplexity: 19.6219\n",
      "Epoch [1/5], Step [900/12942], Loss: 2.7848, Perplexity: 16.19660\n",
      "Epoch [1/5], Step [1000/12942], Loss: 3.0391, Perplexity: 20.8860\n",
      "Epoch [1/5], Step [1100/12942], Loss: 3.2393, Perplexity: 25.5147\n",
      "Epoch [1/5], Step [1200/12942], Loss: 2.8198, Perplexity: 16.7739\n",
      "Epoch [1/5], Step [1300/12942], Loss: 2.6683, Perplexity: 14.41497\n",
      "Epoch [1/5], Step [1400/12942], Loss: 2.5631, Perplexity: 12.9760\n",
      "Epoch [1/5], Step [1500/12942], Loss: 2.8517, Perplexity: 17.3170\n",
      "Epoch [1/5], Step [1600/12942], Loss: 2.5009, Perplexity: 12.1929\n",
      "Epoch [1/5], Step [1700/12942], Loss: 2.5463, Perplexity: 12.7601\n",
      "Epoch [1/5], Step [1800/12942], Loss: 3.0257, Perplexity: 20.6087\n",
      "Epoch [1/5], Step [1900/12942], Loss: 2.6957, Perplexity: 14.8159\n",
      "Epoch [1/5], Step [2000/12942], Loss: 2.6943, Perplexity: 14.7947\n",
      "Epoch [1/5], Step [2100/12942], Loss: 2.6989, Perplexity: 14.8635\n",
      "Epoch [1/5], Step [2200/12942], Loss: 2.8397, Perplexity: 17.1112\n",
      "Epoch [1/5], Step [2300/12942], Loss: 2.3425, Perplexity: 10.4074\n",
      "Epoch [1/5], Step [2400/12942], Loss: 2.7019, Perplexity: 14.9076\n",
      "Epoch [1/5], Step [2500/12942], Loss: 2.4592, Perplexity: 11.6959\n",
      "Epoch [1/5], Step [2600/12942], Loss: 2.7647, Perplexity: 15.8739\n",
      "Epoch [1/5], Step [2700/12942], Loss: 2.2659, Perplexity: 9.63990\n",
      "Epoch [1/5], Step [2800/12942], Loss: 2.3463, Perplexity: 10.4464\n",
      "Epoch [1/5], Step [2900/12942], Loss: 2.3830, Perplexity: 10.8369\n",
      "Epoch [1/5], Step [3000/12942], Loss: 2.4490, Perplexity: 11.5767\n",
      "Epoch [1/5], Step [3100/12942], Loss: 2.2416, Perplexity: 9.40820\n",
      "Epoch [1/5], Step [3200/12942], Loss: 2.6737, Perplexity: 14.4938\n",
      "Epoch [1/5], Step [3300/12942], Loss: 2.8702, Perplexity: 17.6404\n",
      "Epoch [1/5], Step [3400/12942], Loss: 2.5738, Perplexity: 13.1150\n",
      "Epoch [1/5], Step [3500/12942], Loss: 2.3585, Perplexity: 10.5756\n",
      "Epoch [1/5], Step [3600/12942], Loss: 2.5598, Perplexity: 12.9335\n",
      "Epoch [1/5], Step [3700/12942], Loss: 2.2589, Perplexity: 9.57216\n",
      "Epoch [1/5], Step [3800/12942], Loss: 2.4831, Perplexity: 11.9784\n",
      "Epoch [1/5], Step [3900/12942], Loss: 2.3549, Perplexity: 10.5376\n",
      "Epoch [1/5], Step [4000/12942], Loss: 2.4333, Perplexity: 11.3970\n",
      "Epoch [1/5], Step [4100/12942], Loss: 2.4746, Perplexity: 11.8770\n",
      "Epoch [1/5], Step [4200/12942], Loss: 2.6162, Perplexity: 13.6840\n",
      "Epoch [1/5], Step [4300/12942], Loss: 2.2624, Perplexity: 9.60627\n",
      "Epoch [1/5], Step [4400/12942], Loss: 2.4449, Perplexity: 11.5291\n",
      "Epoch [1/5], Step [4500/12942], Loss: 2.4848, Perplexity: 11.9992\n",
      "Epoch [1/5], Step [4600/12942], Loss: 2.1906, Perplexity: 8.94073\n",
      "Epoch [1/5], Step [4700/12942], Loss: 2.4602, Perplexity: 11.7066\n",
      "Epoch [1/5], Step [4800/12942], Loss: 2.4100, Perplexity: 11.1337\n",
      "Epoch [1/5], Step [4900/12942], Loss: 2.2197, Perplexity: 9.20424\n",
      "Epoch [1/5], Step [5000/12942], Loss: 2.0927, Perplexity: 8.10659\n",
      "Epoch [1/5], Step [5100/12942], Loss: 2.5352, Perplexity: 12.6195\n",
      "Epoch [1/5], Step [5200/12942], Loss: 2.4054, Perplexity: 11.0824\n",
      "Epoch [1/5], Step [5300/12942], Loss: 2.2548, Perplexity: 9.53387\n",
      "Epoch [1/5], Step [5400/12942], Loss: 2.4641, Perplexity: 11.7530\n",
      "Epoch [1/5], Step [5500/12942], Loss: 2.1456, Perplexity: 8.54705\n",
      "Epoch [1/5], Step [5600/12942], Loss: 2.3095, Perplexity: 10.0697\n",
      "Epoch [1/5], Step [5700/12942], Loss: 2.2878, Perplexity: 9.85363\n",
      "Epoch [1/5], Step [5800/12942], Loss: 2.3907, Perplexity: 10.9211\n",
      "Epoch [1/5], Step [5900/12942], Loss: 2.7788, Perplexity: 16.0990\n",
      "Epoch [1/5], Step [6000/12942], Loss: 2.1855, Perplexity: 8.89491\n",
      "Epoch [1/5], Step [6100/12942], Loss: 2.0376, Perplexity: 7.67247\n",
      "Epoch [1/5], Step [6200/12942], Loss: 2.6344, Perplexity: 13.9355\n",
      "Epoch [1/5], Step [6300/12942], Loss: 2.2053, Perplexity: 9.07290\n",
      "Epoch [1/5], Step [6400/12942], Loss: 2.3505, Perplexity: 10.4908\n",
      "Epoch [1/5], Step [6500/12942], Loss: 2.1873, Perplexity: 8.91149\n",
      "Epoch [1/5], Step [6600/12942], Loss: 1.9732, Perplexity: 7.19334\n",
      "Epoch [1/5], Step [6700/12942], Loss: 2.3997, Perplexity: 11.02032\n",
      "Epoch [1/5], Step [6800/12942], Loss: 1.8052, Perplexity: 6.08136\n",
      "Epoch [1/5], Step [6900/12942], Loss: 2.4491, Perplexity: 11.5782\n",
      "Epoch [1/5], Step [7000/12942], Loss: 2.8061, Perplexity: 16.5455\n",
      "Epoch [1/5], Step [7100/12942], Loss: 2.1923, Perplexity: 8.95595\n",
      "Epoch [1/5], Step [7200/12942], Loss: 2.2348, Perplexity: 9.34442\n",
      "Epoch [1/5], Step [7300/12942], Loss: 2.1147, Perplexity: 8.28671\n",
      "Epoch [1/5], Step [7400/12942], Loss: 2.7825, Perplexity: 16.1599\n",
      "Epoch [1/5], Step [7500/12942], Loss: 2.2144, Perplexity: 9.15554\n",
      "Epoch [1/5], Step [7600/12942], Loss: 2.0415, Perplexity: 7.70183\n",
      "Epoch [1/5], Step [7700/12942], Loss: 2.2870, Perplexity: 9.84570\n",
      "Epoch [1/5], Step [7800/12942], Loss: 2.1637, Perplexity: 8.70367\n",
      "Epoch [1/5], Step [7900/12942], Loss: 2.2940, Perplexity: 9.91406\n",
      "Epoch [1/5], Step [8000/12942], Loss: 2.3185, Perplexity: 10.1606\n",
      "Epoch [1/5], Step [8100/12942], Loss: 2.5317, Perplexity: 12.5752\n",
      "Epoch [1/5], Step [8200/12942], Loss: 2.0682, Perplexity: 7.910295\n",
      "Epoch [1/5], Step [8300/12942], Loss: 2.3000, Perplexity: 9.974798\n",
      "Epoch [1/5], Step [8400/12942], Loss: 2.2432, Perplexity: 9.42335\n",
      "Epoch [1/5], Step [8500/12942], Loss: 2.4478, Perplexity: 11.5630\n",
      "Epoch [1/5], Step [8600/12942], Loss: 2.2666, Perplexity: 9.64670\n",
      "Epoch [1/5], Step [8700/12942], Loss: 2.2128, Perplexity: 9.14128\n",
      "Epoch [1/5], Step [8800/12942], Loss: 2.7814, Perplexity: 16.1410\n",
      "Epoch [1/5], Step [8900/12942], Loss: 2.3017, Perplexity: 9.99140\n",
      "Epoch [1/5], Step [9000/12942], Loss: 2.3814, Perplexity: 10.8198\n",
      "Epoch [1/5], Step [9100/12942], Loss: 1.9173, Perplexity: 6.80275\n",
      "Epoch [1/5], Step [9200/12942], Loss: 2.4686, Perplexity: 11.8055\n",
      "Epoch [1/5], Step [9300/12942], Loss: 2.3897, Perplexity: 10.9105\n",
      "Epoch [1/5], Step [9400/12942], Loss: 2.1618, Perplexity: 8.68694\n",
      "Epoch [1/5], Step [9500/12942], Loss: 2.3449, Perplexity: 10.4325\n",
      "Epoch [1/5], Step [9600/12942], Loss: 2.3093, Perplexity: 10.0673\n",
      "Epoch [1/5], Step [9700/12942], Loss: 2.3517, Perplexity: 10.5035\n",
      "Epoch [1/5], Step [9800/12942], Loss: 2.2770, Perplexity: 9.74738\n",
      "Epoch [1/5], Step [9900/12942], Loss: 2.2384, Perplexity: 9.37824\n",
      "Epoch [1/5], Step [10000/12942], Loss: 2.1988, Perplexity: 9.0138\n",
      "Epoch [1/5], Step [10100/12942], Loss: 1.9200, Perplexity: 6.82109\n",
      "Epoch [1/5], Step [10200/12942], Loss: 2.1407, Perplexity: 8.50525\n",
      "Epoch [1/5], Step [10300/12942], Loss: 3.1943, Perplexity: 24.39367\n",
      "Epoch [1/5], Step [10400/12942], Loss: 2.0533, Perplexity: 7.79363\n",
      "Epoch [1/5], Step [10500/12942], Loss: 2.1712, Perplexity: 8.76886\n",
      "Epoch [1/5], Step [10600/12942], Loss: 2.4569, Perplexity: 11.6681\n",
      "Epoch [1/5], Step [10700/12942], Loss: 2.3506, Perplexity: 10.4918\n",
      "Epoch [1/5], Step [10800/12942], Loss: 1.8744, Perplexity: 6.51688\n",
      "Epoch [1/5], Step [10900/12942], Loss: 2.1294, Perplexity: 8.40969\n",
      "Epoch [1/5], Step [11000/12942], Loss: 2.1120, Perplexity: 8.26461\n",
      "Epoch [1/5], Step [11100/12942], Loss: 2.5704, Perplexity: 13.0706\n",
      "Epoch [1/5], Step [11200/12942], Loss: 2.3709, Perplexity: 10.7065\n",
      "Epoch [1/5], Step [11300/12942], Loss: 3.1403, Perplexity: 23.1113\n",
      "Epoch [1/5], Step [11400/12942], Loss: 1.9270, Perplexity: 6.86877\n",
      "Epoch [1/5], Step [11500/12942], Loss: 1.9844, Perplexity: 7.27480\n",
      "Epoch [1/5], Step [11600/12942], Loss: 2.5300, Perplexity: 12.5534\n",
      "Epoch [1/5], Step [11700/12942], Loss: 2.6502, Perplexity: 14.1562\n",
      "Epoch [1/5], Step [11800/12942], Loss: 2.4369, Perplexity: 11.4377\n",
      "Epoch [1/5], Step [11900/12942], Loss: 1.9613, Perplexity: 7.10895\n",
      "Epoch [1/5], Step [12000/12942], Loss: 2.0283, Perplexity: 7.60095\n",
      "Epoch [1/5], Step [12100/12942], Loss: 1.9541, Perplexity: 7.05747\n",
      "Epoch [1/5], Step [12200/12942], Loss: 3.1420, Perplexity: 23.1504\n",
      "Epoch [1/5], Step [12300/12942], Loss: 2.0510, Perplexity: 7.77593\n",
      "Epoch [1/5], Step [12400/12942], Loss: 2.5604, Perplexity: 12.9413\n",
      "Epoch [1/5], Step [12500/12942], Loss: 2.2805, Perplexity: 9.78132\n",
      "Epoch [1/5], Step [12600/12942], Loss: 2.0713, Perplexity: 7.93510\n",
      "Epoch [1/5], Step [12700/12942], Loss: 2.1167, Perplexity: 8.30373\n",
      "Epoch [1/5], Step [12800/12942], Loss: 1.9462, Perplexity: 7.00181\n",
      "Epoch [1/5], Step [12900/12942], Loss: 2.2583, Perplexity: 9.56631\n",
      "Epoch [2/5], Step [100/12942], Loss: 2.0566, Perplexity: 7.8196021\n",
      "Epoch [2/5], Step [200/12942], Loss: 2.1307, Perplexity: 8.42080\n",
      "Epoch [2/5], Step [300/12942], Loss: 2.2288, Perplexity: 9.28846\n",
      "Epoch [2/5], Step [400/12942], Loss: 2.1757, Perplexity: 8.80818\n",
      "Epoch [2/5], Step [500/12942], Loss: 2.0846, Perplexity: 8.04144\n",
      "Epoch [2/5], Step [600/12942], Loss: 1.9752, Perplexity: 7.20798\n",
      "Epoch [2/5], Step [700/12942], Loss: 2.0861, Perplexity: 8.05373\n",
      "Epoch [2/5], Step [800/12942], Loss: 2.0822, Perplexity: 8.02203\n",
      "Epoch [2/5], Step [900/12942], Loss: 2.3591, Perplexity: 10.5816\n",
      "Epoch [2/5], Step [1000/12942], Loss: 2.3441, Perplexity: 10.4240\n",
      "Epoch [2/5], Step [1100/12942], Loss: 2.0040, Perplexity: 7.41874\n",
      "Epoch [2/5], Step [1200/12942], Loss: 2.2454, Perplexity: 9.44468\n",
      "Epoch [2/5], Step [1300/12942], Loss: 2.2041, Perplexity: 9.06185\n",
      "Epoch [2/5], Step [1400/12942], Loss: 1.8078, Perplexity: 6.09687\n",
      "Epoch [2/5], Step [1500/12942], Loss: 2.1812, Perplexity: 8.85707\n",
      "Epoch [2/5], Step [1600/12942], Loss: 2.2573, Perplexity: 9.55771\n",
      "Epoch [2/5], Step [1700/12942], Loss: 2.2949, Perplexity: 9.92372\n",
      "Epoch [2/5], Step [1800/12942], Loss: 2.1503, Perplexity: 8.58716\n",
      "Epoch [2/5], Step [1900/12942], Loss: 2.3427, Perplexity: 10.4093\n",
      "Epoch [2/5], Step [2000/12942], Loss: 2.0803, Perplexity: 8.00729\n",
      "Epoch [2/5], Step [2100/12942], Loss: 2.2670, Perplexity: 9.65091\n",
      "Epoch [2/5], Step [2200/12942], Loss: 2.1995, Perplexity: 9.02047\n",
      "Epoch [2/5], Step [2300/12942], Loss: 2.2617, Perplexity: 9.59975\n",
      "Epoch [2/5], Step [2400/12942], Loss: 2.0400, Perplexity: 7.69030\n",
      "Epoch [2/5], Step [2500/12942], Loss: 2.2642, Perplexity: 9.62387\n",
      "Epoch [2/5], Step [2600/12942], Loss: 2.3364, Perplexity: 10.3441\n",
      "Epoch [2/5], Step [2700/12942], Loss: 2.1747, Perplexity: 8.79924\n",
      "Epoch [2/5], Step [2800/12942], Loss: 2.9507, Perplexity: 19.1189\n",
      "Epoch [2/5], Step [2900/12942], Loss: 1.9810, Perplexity: 7.25001\n",
      "Epoch [2/5], Step [3000/12942], Loss: 2.1364, Perplexity: 8.46918\n",
      "Epoch [2/5], Step [3100/12942], Loss: 2.0051, Perplexity: 7.42699\n",
      "Epoch [2/5], Step [3200/12942], Loss: 1.9980, Perplexity: 7.37412\n",
      "Epoch [2/5], Step [3300/12942], Loss: 1.9676, Perplexity: 7.15367\n",
      "Epoch [2/5], Step [3400/12942], Loss: 2.1780, Perplexity: 8.82866\n",
      "Epoch [2/5], Step [3500/12942], Loss: 2.1243, Perplexity: 8.36704\n",
      "Epoch [2/5], Step [3600/12942], Loss: 2.0940, Perplexity: 8.11707\n",
      "Epoch [2/5], Step [3700/12942], Loss: 2.1350, Perplexity: 8.45663\n",
      "Epoch [2/5], Step [3800/12942], Loss: 2.1714, Perplexity: 8.77079\n",
      "Epoch [2/5], Step [4500/12942], Loss: 2.6351, Perplexity: 13.9450\n",
      "Epoch [2/5], Step [4600/12942], Loss: 1.7694, Perplexity: 5.86747\n",
      "Epoch [2/5], Step [4700/12942], Loss: 2.0837, Perplexity: 8.03431\n",
      "Epoch [2/5], Step [4800/12942], Loss: 2.1829, Perplexity: 8.87215\n",
      "Epoch [2/5], Step [4900/12942], Loss: 2.2328, Perplexity: 9.32618\n",
      "Epoch [2/5], Step [5000/12942], Loss: 2.1211, Perplexity: 8.34029\n",
      "Epoch [2/5], Step [5100/12942], Loss: 2.1315, Perplexity: 8.42758\n",
      "Epoch [2/5], Step [5200/12942], Loss: 1.9745, Perplexity: 7.20284\n",
      "Epoch [2/5], Step [5300/12942], Loss: 1.9971, Perplexity: 7.368042\n",
      "Epoch [2/5], Step [5400/12942], Loss: 2.1698, Perplexity: 8.75709\n",
      "Epoch [2/5], Step [5500/12942], Loss: 2.7495, Perplexity: 15.6343\n",
      "Epoch [2/5], Step [5600/12942], Loss: 2.0334, Perplexity: 7.63995\n",
      "Epoch [2/5], Step [5700/12942], Loss: 2.1525, Perplexity: 8.60679\n",
      "Epoch [2/5], Step [5800/12942], Loss: 2.1122, Perplexity: 8.26670\n",
      "Epoch [2/5], Step [5900/12942], Loss: 2.1164, Perplexity: 8.30134\n",
      "Epoch [2/5], Step [6000/12942], Loss: 1.9848, Perplexity: 7.27790\n",
      "Epoch [2/5], Step [6100/12942], Loss: 1.9734, Perplexity: 7.19516\n",
      "Epoch [2/5], Step [6200/12942], Loss: 1.8860, Perplexity: 6.59271\n",
      "Epoch [2/5], Step [6300/12942], Loss: 2.0582, Perplexity: 7.83189\n",
      "Epoch [2/5], Step [6400/12942], Loss: 2.1817, Perplexity: 8.86166\n",
      "Epoch [2/5], Step [6500/12942], Loss: 2.1075, Perplexity: 8.22792\n",
      "Epoch [2/5], Step [6600/12942], Loss: 2.0163, Perplexity: 7.51031\n",
      "Epoch [2/5], Step [6700/12942], Loss: 1.9669, Perplexity: 7.14828\n",
      "Epoch [2/5], Step [6800/12942], Loss: 1.9898, Perplexity: 7.31393\n",
      "Epoch [2/5], Step [6900/12942], Loss: 2.2078, Perplexity: 9.09561\n",
      "Epoch [2/5], Step [7000/12942], Loss: 2.0808, Perplexity: 8.01091\n",
      "Epoch [2/5], Step [7100/12942], Loss: 2.1464, Perplexity: 8.55360\n",
      "Epoch [2/5], Step [7200/12942], Loss: 2.0243, Perplexity: 7.57104\n",
      "Epoch [2/5], Step [7300/12942], Loss: 2.2257, Perplexity: 9.26017\n",
      "Epoch [2/5], Step [7400/12942], Loss: 2.1849, Perplexity: 8.89010\n",
      "Epoch [2/5], Step [7500/12942], Loss: 2.2345, Perplexity: 9.34154\n",
      "Epoch [2/5], Step [7600/12942], Loss: 2.6866, Perplexity: 14.6821\n",
      "Epoch [2/5], Step [7700/12942], Loss: 1.8992, Perplexity: 6.68053\n",
      "Epoch [2/5], Step [7800/12942], Loss: 2.0758, Perplexity: 7.97113\n",
      "Epoch [2/5], Step [7900/12942], Loss: 2.1348, Perplexity: 8.45512\n",
      "Epoch [2/5], Step [8000/12942], Loss: 1.8872, Perplexity: 6.60061\n",
      "Epoch [2/5], Step [8100/12942], Loss: 2.0736, Perplexity: 7.95356\n",
      "Epoch [2/5], Step [8200/12942], Loss: 2.0116, Perplexity: 7.47538\n",
      "Epoch [2/5], Step [8300/12942], Loss: 2.0821, Perplexity: 8.02103\n",
      "Epoch [2/5], Step [8400/12942], Loss: 1.9260, Perplexity: 6.86176\n",
      "Epoch [2/5], Step [8500/12942], Loss: 2.0330, Perplexity: 7.63725\n",
      "Epoch [2/5], Step [8600/12942], Loss: 2.0392, Perplexity: 7.68456\n",
      "Epoch [2/5], Step [8700/12942], Loss: 2.0600, Perplexity: 7.84606\n",
      "Epoch [2/5], Step [8800/12942], Loss: 1.8991, Perplexity: 6.67973\n",
      "Epoch [2/5], Step [8900/12942], Loss: 2.2148, Perplexity: 9.15958\n",
      "Epoch [2/5], Step [9000/12942], Loss: 1.9170, Perplexity: 6.80037\n",
      "Epoch [2/5], Step [9100/12942], Loss: 2.1076, Perplexity: 8.22834\n",
      "Epoch [2/5], Step [9200/12942], Loss: 2.1020, Perplexity: 8.18247\n",
      "Epoch [2/5], Step [9300/12942], Loss: 2.1026, Perplexity: 8.18782\n",
      "Epoch [2/5], Step [9400/12942], Loss: 2.4014, Perplexity: 11.0384\n",
      "Epoch [2/5], Step [9500/12942], Loss: 1.8920, Perplexity: 6.63260\n",
      "Epoch [2/5], Step [9600/12942], Loss: 1.8866, Perplexity: 6.59672\n",
      "Epoch [2/5], Step [9700/12942], Loss: 1.9888, Perplexity: 7.30692\n",
      "Epoch [2/5], Step [9800/12942], Loss: 2.1354, Perplexity: 8.46060\n",
      "Epoch [2/5], Step [9900/12942], Loss: 2.1465, Perplexity: 8.55467\n",
      "Epoch [2/5], Step [10000/12942], Loss: 2.7885, Perplexity: 16.2567\n",
      "Epoch [2/5], Step [10100/12942], Loss: 2.1293, Perplexity: 8.40945\n",
      "Epoch [2/5], Step [10200/12942], Loss: 2.1255, Perplexity: 8.37671\n",
      "Epoch [2/5], Step [10300/12942], Loss: 2.0787, Perplexity: 7.99432\n",
      "Epoch [2/5], Step [10400/12942], Loss: 1.9687, Perplexity: 7.16171\n",
      "Epoch [2/5], Step [10500/12942], Loss: 1.8307, Perplexity: 6.23838\n",
      "Epoch [2/5], Step [10600/12942], Loss: 1.9457, Perplexity: 6.99833\n",
      "Epoch [2/5], Step [10700/12942], Loss: 1.9142, Perplexity: 6.78139\n",
      "Epoch [2/5], Step [10800/12942], Loss: 2.0508, Perplexity: 7.77431\n",
      "Epoch [2/5], Step [10900/12942], Loss: 2.6177, Perplexity: 13.7040\n",
      "Epoch [2/5], Step [11000/12942], Loss: 1.9048, Perplexity: 6.71794\n",
      "Epoch [2/5], Step [11100/12942], Loss: 2.2166, Perplexity: 9.17640\n",
      "Epoch [2/5], Step [11200/12942], Loss: 2.3488, Perplexity: 10.4731\n",
      "Epoch [2/5], Step [11300/12942], Loss: 2.0700, Perplexity: 7.92466\n",
      "Epoch [2/5], Step [11400/12942], Loss: 2.0925, Perplexity: 8.10538\n",
      "Epoch [2/5], Step [11500/12942], Loss: 2.0666, Perplexity: 7.89817\n",
      "Epoch [2/5], Step [11600/12942], Loss: 2.1323, Perplexity: 8.43393\n",
      "Epoch [2/5], Step [11700/12942], Loss: 2.1538, Perplexity: 8.61778\n",
      "Epoch [2/5], Step [11800/12942], Loss: 1.6274, Perplexity: 5.09093\n",
      "Epoch [2/5], Step [11900/12942], Loss: 2.0944, Perplexity: 8.12061\n",
      "Epoch [2/5], Step [12000/12942], Loss: 1.7927, Perplexity: 6.00571\n",
      "Epoch [2/5], Step [12100/12942], Loss: 1.9552, Perplexity: 7.06560\n",
      "Epoch [2/5], Step [12200/12942], Loss: 2.1779, Perplexity: 8.82793\n",
      "Epoch [2/5], Step [12300/12942], Loss: 2.0642, Perplexity: 7.87870\n",
      "Epoch [2/5], Step [12400/12942], Loss: 2.1751, Perplexity: 8.80307\n",
      "Epoch [2/5], Step [12500/12942], Loss: 2.0619, Perplexity: 7.86093\n",
      "Epoch [2/5], Step [12600/12942], Loss: 2.1945, Perplexity: 8.97537\n",
      "Epoch [2/5], Step [12700/12942], Loss: 2.0671, Perplexity: 7.90226\n",
      "Epoch [2/5], Step [12800/12942], Loss: 2.4430, Perplexity: 11.5075\n",
      "Epoch [2/5], Step [12900/12942], Loss: 2.0316, Perplexity: 7.62640\n",
      "Epoch [3/5], Step [100/12942], Loss: 1.9315, Perplexity: 6.8997867\n",
      "Epoch [3/5], Step [200/12942], Loss: 1.9775, Perplexity: 7.22475\n",
      "Epoch [3/5], Step [300/12942], Loss: 2.0741, Perplexity: 7.95747\n",
      "Epoch [3/5], Step [400/12942], Loss: 2.1681, Perplexity: 8.74204\n",
      "Epoch [3/5], Step [500/12942], Loss: 1.9044, Perplexity: 6.71572\n",
      "Epoch [3/5], Step [600/12942], Loss: 2.1313, Perplexity: 8.42579\n",
      "Epoch [3/5], Step [700/12942], Loss: 1.9023, Perplexity: 6.70100\n",
      "Epoch [3/5], Step [800/12942], Loss: 1.9361, Perplexity: 6.93136\n",
      "Epoch [3/5], Step [900/12942], Loss: 1.8389, Perplexity: 6.28973\n",
      "Epoch [3/5], Step [1000/12942], Loss: 1.9037, Perplexity: 6.7108\n",
      "Epoch [3/5], Step [1100/12942], Loss: 1.9741, Perplexity: 7.20046\n",
      "Epoch [3/5], Step [1200/12942], Loss: 2.0611, Perplexity: 7.85469\n",
      "Epoch [3/5], Step [1300/12942], Loss: 2.3600, Perplexity: 10.5905\n",
      "Epoch [3/5], Step [1400/12942], Loss: 2.0350, Perplexity: 7.65253\n",
      "Epoch [3/5], Step [1500/12942], Loss: 2.0547, Perplexity: 7.80460\n",
      "Epoch [3/5], Step [1600/12942], Loss: 1.8571, Perplexity: 6.40529\n",
      "Epoch [3/5], Step [1700/12942], Loss: 2.6968, Perplexity: 14.8327\n",
      "Epoch [3/5], Step [1800/12942], Loss: 2.0116, Perplexity: 7.475358\n",
      "Epoch [3/5], Step [1900/12942], Loss: 2.2912, Perplexity: 9.88713\n",
      "Epoch [3/5], Step [2000/12942], Loss: 1.8893, Perplexity: 6.61452\n",
      "Epoch [3/5], Step [2100/12942], Loss: 2.1982, Perplexity: 9.00855\n",
      "Epoch [3/5], Step [2200/12942], Loss: 2.0369, Perplexity: 7.66651\n",
      "Epoch [3/5], Step [2300/12942], Loss: 2.5820, Perplexity: 13.2237\n",
      "Epoch [3/5], Step [2400/12942], Loss: 2.1175, Perplexity: 8.31020\n",
      "Epoch [3/5], Step [2500/12942], Loss: 2.0541, Perplexity: 7.79985\n",
      "Epoch [3/5], Step [2600/12942], Loss: 2.2330, Perplexity: 9.32748\n",
      "Epoch [3/5], Step [2700/12942], Loss: 2.0567, Perplexity: 7.82046\n",
      "Epoch [3/5], Step [2800/12942], Loss: 1.7263, Perplexity: 5.62009\n",
      "Epoch [3/5], Step [2900/12942], Loss: 1.9705, Perplexity: 7.17450\n",
      "Epoch [3/5], Step [3000/12942], Loss: 1.9556, Perplexity: 7.06841\n",
      "Epoch [3/5], Step [3100/12942], Loss: 2.0227, Perplexity: 7.55913\n",
      "Epoch [3/5], Step [3200/12942], Loss: 2.0201, Perplexity: 7.53882\n",
      "Epoch [3/5], Step [3300/12942], Loss: 1.9949, Perplexity: 7.35127\n",
      "Epoch [3/5], Step [3400/12942], Loss: 2.3626, Perplexity: 10.6183\n",
      "Epoch [3/5], Step [3500/12942], Loss: 2.4806, Perplexity: 11.9483\n",
      "Epoch [3/5], Step [3600/12942], Loss: 2.1058, Perplexity: 8.21403\n",
      "Epoch [3/5], Step [3700/12942], Loss: 2.2086, Perplexity: 9.10309\n",
      "Epoch [3/5], Step [3800/12942], Loss: 2.2679, Perplexity: 9.65870\n",
      "Epoch [3/5], Step [3900/12942], Loss: 2.0733, Perplexity: 7.95096\n",
      "Epoch [3/5], Step [4000/12942], Loss: 1.9993, Perplexity: 7.38413\n",
      "Epoch [3/5], Step [4100/12942], Loss: 2.0396, Perplexity: 7.68787\n",
      "Epoch [3/5], Step [4200/12942], Loss: 2.1377, Perplexity: 8.48035\n",
      "Epoch [3/5], Step [4300/12942], Loss: 1.8363, Perplexity: 6.27335\n",
      "Epoch [3/5], Step [4400/12942], Loss: 2.0869, Perplexity: 8.05969\n",
      "Epoch [3/5], Step [4500/12942], Loss: 2.0168, Perplexity: 7.51404\n",
      "Epoch [3/5], Step [4600/12942], Loss: 1.9657, Perplexity: 7.13977\n",
      "Epoch [3/5], Step [4700/12942], Loss: 1.7352, Perplexity: 5.67023\n",
      "Epoch [3/5], Step [4800/12942], Loss: 2.0293, Perplexity: 7.60897\n",
      "Epoch [3/5], Step [4900/12942], Loss: 1.9451, Perplexity: 6.99447\n",
      "Epoch [3/5], Step [5000/12942], Loss: 1.7090, Perplexity: 5.52345\n",
      "Epoch [3/5], Step [5100/12942], Loss: 2.1306, Perplexity: 8.42013\n",
      "Epoch [3/5], Step [5200/12942], Loss: 2.2649, Perplexity: 9.62984\n",
      "Epoch [3/5], Step [5300/12942], Loss: 2.4842, Perplexity: 11.9919\n",
      "Epoch [3/5], Step [5400/12942], Loss: 2.1700, Perplexity: 8.758435\n",
      "Epoch [3/5], Step [5500/12942], Loss: 2.0352, Perplexity: 7.65360\n",
      "Epoch [3/5], Step [5600/12942], Loss: 2.2318, Perplexity: 9.31635\n",
      "Epoch [3/5], Step [5700/12942], Loss: 2.1417, Perplexity: 8.51360\n",
      "Epoch [3/5], Step [5800/12942], Loss: 2.2364, Perplexity: 9.36005\n",
      "Epoch [3/5], Step [5900/12942], Loss: 2.1768, Perplexity: 8.81805\n",
      "Epoch [3/5], Step [6000/12942], Loss: 2.1156, Perplexity: 8.29438\n",
      "Epoch [3/5], Step [6100/12942], Loss: 2.8100, Perplexity: 16.6107\n",
      "Epoch [3/5], Step [6200/12942], Loss: 2.0419, Perplexity: 7.70555\n",
      "Epoch [3/5], Step [6300/12942], Loss: 1.8645, Perplexity: 6.45281\n",
      "Epoch [3/5], Step [6400/12942], Loss: 1.8617, Perplexity: 6.43489\n",
      "Epoch [3/5], Step [6500/12942], Loss: 1.8545, Perplexity: 6.38881\n",
      "Epoch [3/5], Step [6600/12942], Loss: 2.0524, Perplexity: 7.78656\n",
      "Epoch [3/5], Step [6700/12942], Loss: 2.2105, Perplexity: 9.12066\n",
      "Epoch [3/5], Step [6800/12942], Loss: 2.0666, Perplexity: 7.89800\n",
      "Epoch [3/5], Step [6900/12942], Loss: 2.2200, Perplexity: 9.20777\n",
      "Epoch [3/5], Step [7000/12942], Loss: 2.1187, Perplexity: 8.32035\n",
      "Epoch [3/5], Step [7100/12942], Loss: 2.3826, Perplexity: 10.8333\n",
      "Epoch [3/5], Step [7200/12942], Loss: 1.9993, Perplexity: 7.38406\n",
      "Epoch [3/5], Step [7300/12942], Loss: 2.0930, Perplexity: 8.10958\n",
      "Epoch [3/5], Step [7400/12942], Loss: 2.3443, Perplexity: 10.4261\n",
      "Epoch [3/5], Step [7500/12942], Loss: 1.9266, Perplexity: 6.86589\n",
      "Epoch [3/5], Step [7600/12942], Loss: 2.0267, Perplexity: 7.58879\n",
      "Epoch [3/5], Step [7700/12942], Loss: 2.4192, Perplexity: 11.2372\n",
      "Epoch [3/5], Step [7800/12942], Loss: 1.7890, Perplexity: 5.98377\n",
      "Epoch [3/5], Step [7900/12942], Loss: 1.8207, Perplexity: 6.17615\n",
      "Epoch [3/5], Step [8000/12942], Loss: 1.9269, Perplexity: 6.86839\n",
      "Epoch [3/5], Step [8100/12942], Loss: 2.0725, Perplexity: 7.94433\n",
      "Epoch [3/5], Step [8200/12942], Loss: 1.8906, Perplexity: 6.62355\n",
      "Epoch [3/5], Step [8300/12942], Loss: 2.4591, Perplexity: 11.69462\n",
      "Epoch [3/5], Step [8400/12942], Loss: 1.9825, Perplexity: 7.26064\n",
      "Epoch [3/5], Step [8500/12942], Loss: 2.0512, Perplexity: 7.77695\n",
      "Epoch [3/5], Step [8600/12942], Loss: 2.1371, Perplexity: 8.47493\n",
      "Epoch [3/5], Step [8700/12942], Loss: 2.1074, Perplexity: 8.22684\n",
      "Epoch [3/5], Step [8800/12942], Loss: 1.7626, Perplexity: 5.82772\n",
      "Epoch [3/5], Step [8900/12942], Loss: 1.9825, Perplexity: 7.26064\n",
      "Epoch [3/5], Step [9000/12942], Loss: 1.7935, Perplexity: 6.01050\n",
      "Epoch [3/5], Step [9100/12942], Loss: 2.1090, Perplexity: 8.23970\n",
      "Epoch [3/5], Step [9200/12942], Loss: 1.9541, Perplexity: 7.05731\n",
      "Epoch [3/5], Step [9300/12942], Loss: 2.3291, Perplexity: 10.2684\n",
      "Epoch [3/5], Step [9400/12942], Loss: 1.9996, Perplexity: 7.38621\n",
      "Epoch [3/5], Step [9500/12942], Loss: 1.9088, Perplexity: 6.74518\n",
      "Epoch [3/5], Step [9600/12942], Loss: 1.9402, Perplexity: 6.95994\n",
      "Epoch [3/5], Step [9700/12942], Loss: 2.0273, Perplexity: 7.59321\n",
      "Epoch [3/5], Step [9800/12942], Loss: 1.9553, Perplexity: 7.06603\n",
      "Epoch [3/5], Step [9900/12942], Loss: 1.9209, Perplexity: 6.82731\n",
      "Epoch [3/5], Step [10000/12942], Loss: 1.9607, Perplexity: 7.1043\n",
      "Epoch [3/5], Step [10100/12942], Loss: 1.8532, Perplexity: 6.38028\n",
      "Epoch [3/5], Step [10200/12942], Loss: 1.7534, Perplexity: 5.77437\n",
      "Epoch [3/5], Step [10300/12942], Loss: 1.8208, Perplexity: 6.17675\n",
      "Epoch [3/5], Step [10400/12942], Loss: 2.3032, Perplexity: 10.0065\n",
      "Epoch [3/5], Step [10500/12942], Loss: 2.4014, Perplexity: 11.0390\n",
      "Epoch [3/5], Step [10600/12942], Loss: 1.9226, Perplexity: 6.83907\n",
      "Epoch [3/5], Step [10700/12942], Loss: 1.8359, Perplexity: 6.27116\n",
      "Epoch [3/5], Step [10800/12942], Loss: 2.0363, Perplexity: 7.66195\n",
      "Epoch [3/5], Step [10900/12942], Loss: 2.0338, Perplexity: 7.64326\n",
      "Epoch [3/5], Step [11000/12942], Loss: 1.7006, Perplexity: 5.47751\n",
      "Epoch [3/5], Step [11100/12942], Loss: 2.1627, Perplexity: 8.69422\n",
      "Epoch [3/5], Step [11200/12942], Loss: 1.9121, Perplexity: 6.76709\n",
      "Epoch [3/5], Step [11300/12942], Loss: 2.0885, Perplexity: 8.07257\n",
      "Epoch [3/5], Step [11400/12942], Loss: 2.1243, Perplexity: 8.36696\n",
      "Epoch [3/5], Step [11500/12942], Loss: 2.0499, Perplexity: 7.76734\n",
      "Epoch [3/5], Step [11600/12942], Loss: 1.8934, Perplexity: 6.64196\n",
      "Epoch [3/5], Step [11700/12942], Loss: 2.2555, Perplexity: 9.54027\n",
      "Epoch [3/5], Step [11800/12942], Loss: 1.9823, Perplexity: 7.25942\n",
      "Epoch [3/5], Step [11900/12942], Loss: 2.3172, Perplexity: 10.1472\n",
      "Epoch [3/5], Step [12000/12942], Loss: 2.1881, Perplexity: 8.91791\n",
      "Epoch [3/5], Step [12100/12942], Loss: 2.1724, Perplexity: 8.77975\n",
      "Epoch [3/5], Step [12200/12942], Loss: 2.0539, Perplexity: 7.79842\n",
      "Epoch [3/5], Step [12300/12942], Loss: 2.2252, Perplexity: 9.25494\n",
      "Epoch [3/5], Step [12400/12942], Loss: 2.4237, Perplexity: 11.2874\n",
      "Epoch [3/5], Step [12500/12942], Loss: 2.3188, Perplexity: 10.1636\n",
      "Epoch [3/5], Step [12600/12942], Loss: 2.2989, Perplexity: 9.96326\n",
      "Epoch [3/5], Step [12700/12942], Loss: 2.0985, Perplexity: 8.15395\n",
      "Epoch [3/5], Step [12800/12942], Loss: 1.9285, Perplexity: 6.87921\n",
      "Epoch [3/5], Step [12900/12942], Loss: 2.0855, Perplexity: 8.04867\n",
      "Epoch [4/5], Step [100/12942], Loss: 1.9510, Perplexity: 7.03604981\n",
      "Epoch [4/5], Step [200/12942], Loss: 2.1772, Perplexity: 8.82136\n",
      "Epoch [4/5], Step [300/12942], Loss: 1.9478, Perplexity: 7.01316\n",
      "Epoch [4/5], Step [400/12942], Loss: 2.1489, Perplexity: 8.57561\n",
      "Epoch [4/5], Step [500/12942], Loss: 2.4302, Perplexity: 11.3611\n",
      "Epoch [4/5], Step [600/12942], Loss: 2.1998, Perplexity: 9.02350\n",
      "Epoch [4/5], Step [700/12942], Loss: 2.0695, Perplexity: 7.92055\n",
      "Epoch [4/5], Step [800/12942], Loss: 2.0529, Perplexity: 7.79037\n",
      "Epoch [4/5], Step [900/12942], Loss: 1.8938, Perplexity: 6.64435\n",
      "Epoch [4/5], Step [1000/12942], Loss: 2.2122, Perplexity: 9.1357\n",
      "Epoch [4/5], Step [1100/12942], Loss: 2.3532, Perplexity: 10.5193\n",
      "Epoch [4/5], Step [1200/12942], Loss: 2.1430, Perplexity: 8.52531\n",
      "Epoch [4/5], Step [1300/12942], Loss: 2.1496, Perplexity: 8.58140\n",
      "Epoch [4/5], Step [1400/12942], Loss: 1.8286, Perplexity: 6.22542\n",
      "Epoch [4/5], Step [1500/12942], Loss: 1.9720, Perplexity: 7.18481\n",
      "Epoch [4/5], Step [1600/12942], Loss: 1.9758, Perplexity: 7.21221\n",
      "Epoch [4/5], Step [1700/12942], Loss: 2.7441, Perplexity: 15.5500\n",
      "Epoch [4/5], Step [1800/12942], Loss: 1.9938, Perplexity: 7.34328\n",
      "Epoch [4/5], Step [1900/12942], Loss: 1.9652, Perplexity: 7.13647\n",
      "Epoch [4/5], Step [2000/12942], Loss: 1.7913, Perplexity: 5.99744\n",
      "Epoch [4/5], Step [2100/12942], Loss: 2.0387, Perplexity: 7.68050\n",
      "Epoch [4/5], Step [2200/12942], Loss: 2.0445, Perplexity: 7.72502\n",
      "Epoch [4/5], Step [2300/12942], Loss: 2.0385, Perplexity: 7.67882\n",
      "Epoch [4/5], Step [2400/12942], Loss: 2.1236, Perplexity: 8.36149\n",
      "Epoch [4/5], Step [2500/12942], Loss: 1.8508, Perplexity: 6.36483\n",
      "Epoch [4/5], Step [2600/12942], Loss: 2.0043, Perplexity: 7.42052\n",
      "Epoch [4/5], Step [2700/12942], Loss: 2.1039, Perplexity: 8.19776\n",
      "Epoch [4/5], Step [2800/12942], Loss: 1.8516, Perplexity: 6.37015\n",
      "Epoch [4/5], Step [2900/12942], Loss: 2.1259, Perplexity: 8.38043\n",
      "Epoch [4/5], Step [3000/12942], Loss: 1.9344, Perplexity: 6.92027\n",
      "Epoch [4/5], Step [3100/12942], Loss: 1.8671, Perplexity: 6.46967\n",
      "Epoch [4/5], Step [3200/12942], Loss: 1.7925, Perplexity: 6.00454\n",
      "Epoch [4/5], Step [3300/12942], Loss: 2.0174, Perplexity: 7.51848\n",
      "Epoch [4/5], Step [3400/12942], Loss: 1.8861, Perplexity: 6.59363\n",
      "Epoch [4/5], Step [3500/12942], Loss: 1.8940, Perplexity: 6.64596\n",
      "Epoch [4/5], Step [3600/12942], Loss: 1.8552, Perplexity: 6.39324\n",
      "Epoch [4/5], Step [3700/12942], Loss: 2.0214, Perplexity: 7.54915\n",
      "Epoch [4/5], Step [3800/12942], Loss: 2.1241, Perplexity: 8.36507\n",
      "Epoch [4/5], Step [3900/12942], Loss: 2.3891, Perplexity: 10.9041\n",
      "Epoch [4/5], Step [4000/12942], Loss: 1.9171, Perplexity: 6.801192\n",
      "Epoch [4/5], Step [4100/12942], Loss: 1.8161, Perplexity: 6.14808\n",
      "Epoch [4/5], Step [4200/12942], Loss: 2.3109, Perplexity: 10.0832\n",
      "Epoch [4/5], Step [4300/12942], Loss: 1.8483, Perplexity: 6.34890\n",
      "Epoch [4/5], Step [4400/12942], Loss: 1.9985, Perplexity: 7.37817\n",
      "Epoch [4/5], Step [4500/12942], Loss: 2.0719, Perplexity: 7.94023\n",
      "Epoch [4/5], Step [4600/12942], Loss: 1.7780, Perplexity: 5.91829\n",
      "Epoch [4/5], Step [4700/12942], Loss: 1.8388, Perplexity: 6.28929\n",
      "Epoch [4/5], Step [4800/12942], Loss: 1.9429, Perplexity: 6.97881\n",
      "Epoch [4/5], Step [4900/12942], Loss: 2.0233, Perplexity: 7.56340\n",
      "Epoch [4/5], Step [5000/12942], Loss: 1.9783, Perplexity: 7.23024\n",
      "Epoch [4/5], Step [5100/12942], Loss: 1.8680, Perplexity: 6.47534\n",
      "Epoch [4/5], Step [5200/12942], Loss: 2.1588, Perplexity: 8.66077\n",
      "Epoch [4/5], Step [5300/12942], Loss: 1.9378, Perplexity: 6.94333\n",
      "Epoch [4/5], Step [5400/12942], Loss: 1.9715, Perplexity: 7.18123\n",
      "Epoch [4/5], Step [5500/12942], Loss: 2.3220, Perplexity: 10.1962\n",
      "Epoch [4/5], Step [5600/12942], Loss: 2.6821, Perplexity: 14.6157\n",
      "Epoch [4/5], Step [5700/12942], Loss: 1.7298, Perplexity: 5.63957\n",
      "Epoch [4/5], Step [5800/12942], Loss: 2.1483, Perplexity: 8.57065\n",
      "Epoch [4/5], Step [5900/12942], Loss: 1.9157, Perplexity: 6.79180\n",
      "Epoch [4/5], Step [6000/12942], Loss: 2.0090, Perplexity: 7.45588\n",
      "Epoch [4/5], Step [6100/12942], Loss: 1.8373, Perplexity: 6.27934\n",
      "Epoch [4/5], Step [6200/12942], Loss: 1.7838, Perplexity: 5.95277\n",
      "Epoch [4/5], Step [6300/12942], Loss: 2.0518, Perplexity: 7.78165\n",
      "Epoch [4/5], Step [6400/12942], Loss: 2.0843, Perplexity: 8.03922\n",
      "Epoch [4/5], Step [6500/12942], Loss: 1.8628, Perplexity: 6.44173\n",
      "Epoch [4/5], Step [6600/12942], Loss: 1.9256, Perplexity: 6.85968\n",
      "Epoch [4/5], Step [6700/12942], Loss: 1.9124, Perplexity: 6.76946\n",
      "Epoch [4/5], Step [6800/12942], Loss: 1.8508, Perplexity: 6.36487\n",
      "Epoch [4/5], Step [6900/12942], Loss: 1.5533, Perplexity: 4.72696\n",
      "Epoch [4/5], Step [7000/12942], Loss: 2.0553, Perplexity: 7.80951\n",
      "Epoch [4/5], Step [7100/12942], Loss: 1.7861, Perplexity: 5.96630\n",
      "Epoch [4/5], Step [7200/12942], Loss: 1.7601, Perplexity: 5.81309\n",
      "Epoch [4/5], Step [7300/12942], Loss: 1.9672, Perplexity: 7.15064\n",
      "Epoch [4/5], Step [7400/12942], Loss: 1.8613, Perplexity: 6.43221\n",
      "Epoch [4/5], Step [7500/12942], Loss: 1.9360, Perplexity: 6.93094\n",
      "Epoch [4/5], Step [7600/12942], Loss: 1.7340, Perplexity: 5.66313\n",
      "Epoch [4/5], Step [7700/12942], Loss: 2.1113, Perplexity: 8.25910\n",
      "Epoch [4/5], Step [7800/12942], Loss: 1.9798, Perplexity: 7.24148\n",
      "Epoch [4/5], Step [7900/12942], Loss: 1.6879, Perplexity: 5.40802\n",
      "Epoch [4/5], Step [8000/12942], Loss: 1.6935, Perplexity: 5.43878\n",
      "Epoch [4/5], Step [8100/12942], Loss: 2.1329, Perplexity: 8.43919\n",
      "Epoch [4/5], Step [8200/12942], Loss: 2.0727, Perplexity: 7.94611\n",
      "Epoch [4/5], Step [8300/12942], Loss: 2.1525, Perplexity: 8.60656\n",
      "Epoch [4/5], Step [8400/12942], Loss: 2.2643, Perplexity: 9.62474\n",
      "Epoch [4/5], Step [8500/12942], Loss: 1.8255, Perplexity: 6.20613\n",
      "Epoch [4/5], Step [8600/12942], Loss: 2.2448, Perplexity: 9.43891\n",
      "Epoch [4/5], Step [8700/12942], Loss: 2.0978, Perplexity: 8.14853\n",
      "Epoch [4/5], Step [8800/12942], Loss: 1.8521, Perplexity: 6.37299\n",
      "Epoch [4/5], Step [8900/12942], Loss: 2.1898, Perplexity: 8.93352\n",
      "Epoch [4/5], Step [9000/12942], Loss: 2.0197, Perplexity: 7.53602\n",
      "Epoch [4/5], Step [9100/12942], Loss: 1.9724, Perplexity: 7.18791\n",
      "Epoch [4/5], Step [9200/12942], Loss: 1.8560, Perplexity: 6.39792\n",
      "Epoch [4/5], Step [9300/12942], Loss: 2.0285, Perplexity: 7.60242\n",
      "Epoch [4/5], Step [9400/12942], Loss: 2.1478, Perplexity: 8.56599\n",
      "Epoch [4/5], Step [9500/12942], Loss: 1.9647, Perplexity: 7.13289\n",
      "Epoch [4/5], Step [9600/12942], Loss: 1.9024, Perplexity: 6.70205\n",
      "Epoch [4/5], Step [9700/12942], Loss: 2.1967, Perplexity: 8.99514\n",
      "Epoch [4/5], Step [9800/12942], Loss: 2.6361, Perplexity: 13.9587\n",
      "Epoch [4/5], Step [9900/12942], Loss: 2.0453, Perplexity: 7.73177\n",
      "Epoch [4/5], Step [10000/12942], Loss: 1.7016, Perplexity: 5.4826\n",
      "Epoch [4/5], Step [10100/12942], Loss: 1.7151, Perplexity: 5.55730\n",
      "Epoch [4/5], Step [10200/12942], Loss: 1.9398, Perplexity: 6.95751\n",
      "Epoch [4/5], Step [10300/12942], Loss: 1.9796, Perplexity: 7.23988\n",
      "Epoch [4/5], Step [10400/12942], Loss: 1.8573, Perplexity: 6.406402\n",
      "Epoch [4/5], Step [10500/12942], Loss: 1.9229, Perplexity: 6.84115\n",
      "Epoch [4/5], Step [10600/12942], Loss: 2.0307, Perplexity: 7.61910\n",
      "Epoch [4/5], Step [10700/12942], Loss: 2.0931, Perplexity: 8.11038\n",
      "Epoch [4/5], Step [10800/12942], Loss: 1.8004, Perplexity: 6.05233\n",
      "Epoch [4/5], Step [10900/12942], Loss: 2.0973, Perplexity: 8.14452\n",
      "Epoch [4/5], Step [11000/12942], Loss: 1.8986, Perplexity: 6.67637\n",
      "Epoch [4/5], Step [11095/12942], Loss: 2.0822, Perplexity: 8.02198"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后已经训练完了，没有自动保存，我点保存也没反应直接崩掉，可我的剩余时间已经不够重新跑一遍了，总计18小时，但现在只剩11小时。\n",
    "\n",
    "## 所以这个notebook中无法显示整个训练过程，查看log.txt。同时保存了epoch=5的模型，所以的notebook 3中，我将使用encoder-5.pkl和decoder-5.pkl，即epoch=5的结果。\n",
    "\n",
    "最后Loss是1.8左右。到epoch [5/5] 的时候，Loss是在2.2~1.6之间浮动，而且1.8、1.9出现的频率比2点几要高，经常十来个1.8、1.9之后会出现一个2点几。我觉得差不多也稳定在这个范围了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (可选）验证你的模型\n",
    "\n",
    "为了评估潜在的过度拟合，可以选择评估验证集的性能。如果你决定来做这个**可选**任务，则需要先完成下一个notebook中的所有步骤（**3_Inference.ipynb**）。作为该notebook的一部分，你需要编写并测试使用RNN解码器生成图像标注的代码（特别是`DecoderRNN`类中的`sample`方法）。在这里，该代码会是非常有用的。\n",
    "\n",
    "如果你决定验证模型，请不要在**data_loader.py**中编辑数据加载器。相反，你需要创建一个名为**data_loader_val.py**的新文件，其中包含用于获取验证数据的数据加载器的代码。你可以访问：\n",
    "-  路径为`'/opt/cocoapi/images/train2014/'`的验证图像文件\n",
    "-  路径为`'/opt/cocoapi/annotations/captions_val2014.json'`的文件中 ，用于验证图像标注的注释文件。\n",
    "\n",
    "根据我们的建议，验证模型的方法涉及会到创建一个json文件，例如包含模型预测的验证图像的标注的[这个文件](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) 。然后，你可以编写自己的脚本或使用 [在线查找](https://github.com/tylin/coco-caption) 的脚本来计算模型的BLEU分数。你可以在 [本文](https://arxiv.org/pdf/1411.4555.pdf)第4.1节中阅读有关BLEU分数以及其他评估指标（如TEOR和Cider）的更多信息。有关如何使用注释文件的更多信息，请查看COCO数据集 [网站](http://cocodataset.org/#download) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
